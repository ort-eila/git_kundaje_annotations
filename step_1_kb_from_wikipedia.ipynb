{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ort-eila/git_kundaje_annotations/blob/main/step_1_kb_from_wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41310c68-d624-465b-9c3e-49dffddc1df7",
      "metadata": {
        "id": "41310c68-d624-465b-9c3e-49dffddc1df7"
      },
      "outputs": [],
      "source": [
        "# kb from wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the destination folder in Google Drive\n",
        "destination_folder = '/content/drive/MyDrive/bio-llm/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkuI69gS5gXi",
        "outputId": "85a1c66f-fb88-4b24-8a20-111744538f89"
      },
      "id": "UkuI69gS5gXi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173a6d67-3e71-4567-872b-d3bd7c32d9fc",
      "metadata": {
        "id": "173a6d67-3e71-4567-872b-d3bd7c32d9fc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import pandas as pd\n",
        "import string  # Import the string module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49b9680-ad2e-4107-af84-5b0d990cfa38",
      "metadata": {
        "id": "c49b9680-ad2e-4107-af84-5b0d990cfa38"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_wikipedia(url, txt_filename):\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        res.raise_for_status()\n",
        "        wiki = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        # Extract all paragraphs into a list\n",
        "        paragraphs = [paragraph.getText() for paragraph in wiki.select('p')]\n",
        "\n",
        "        # Join the paragraphs into a single text with a newline character at the end\n",
        "        text = ''.join(paragraphs)\n",
        "        # print(\"text is \",text)\n",
        "        text = text.replace(\"\\n\", \"\")\n",
        "\n",
        "        # Open a file named after the Wikipedia page in write mode\n",
        "        with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text) # + '\\n')\n",
        "\n",
        "        print(f\"Extracted text from '{url}' and saved to '{txt_filename}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0f9f7e-d229-43d3-96f2-dc287dab7d27",
      "metadata": {
        "id": "0f0f9f7e-d229-43d3-96f2-dc287dab7d27"
      },
      "outputs": [],
      "source": [
        "def update_csv_with_wikipedia_details(url_list, csv_filename, remove_wiki_page_list=None):\n",
        "    if os.path.exists(csv_filename):\n",
        "        # Read the existing CSV into a DataFrame with headers\n",
        "        existing_df = pd.read_csv(csv_filename)\n",
        "    else:\n",
        "        # Create an empty DataFrame with headers if the CSV doesn't exist\n",
        "        existing_df = pd.DataFrame(columns=['title', 'text'])\n",
        "\n",
        "    # Create a set of existing titles for fast lookup\n",
        "    existing_titles = set(existing_df['title'])\n",
        "\n",
        "    # Create an empty list to store DataFrames\n",
        "    df_list = []\n",
        "\n",
        "    for wiki_page in url_list:\n",
        "        if wiki_page in existing_titles:\n",
        "            print(f\"Skipping '{wiki_page}' as it already exists in the CSV.\")\n",
        "            continue\n",
        "\n",
        "        # Define the Wikipedia page URL\n",
        "        url = f'https://en.wikipedia.org/wiki/{wiki_page}'\n",
        "\n",
        "        # Define the filename for the text file\n",
        "        txt_filename = f\"{wiki_page}.txt\"\n",
        "\n",
        "        # Call the extract_text_from_wikipedia method to save text to a file\n",
        "        extract_text_from_wikipedia(url, txt_filename)\n",
        "\n",
        "        # Check if the text file was successfully generated\n",
        "        if os.path.exists(txt_filename):\n",
        "            # Read the content from the text file\n",
        "            with open(txt_filename, 'r', encoding='utf-8') as txt_file:\n",
        "                text = txt_file.read()\n",
        "\n",
        "            # Create a DataFrame with the title and text\n",
        "            df = pd.DataFrame({'title': [wiki_page], 'text': [text]})\n",
        "\n",
        "            # Append the DataFrame to the list\n",
        "            df_list.append(df)\n",
        "\n",
        "            # Remove the generated text file\n",
        "            #DEBUG\n",
        "            # os.remove(txt_filename)\n",
        "\n",
        "            print(f\"Downloaded '{wiki_page}' and created DataFrame.\")\n",
        "        else:\n",
        "            print(f\"Failed to download '{wiki_page}'.\")\n",
        "\n",
        "    if df_list:\n",
        "        # Concatenate all DataFrames in the list into one\n",
        "        final_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "        if os.path.exists(csv_filename):\n",
        "            # Append the final DataFrame to the existing CSV file without writing the header\n",
        "            final_df.to_csv(csv_filename, mode='a',sep=\"\\t\" ,header=False, index=False)\n",
        "        else:\n",
        "            # Write the final DataFrame with the header if the CSV doesn't exist\n",
        "            final_df.to_csv(csv_filename, mode='w', sep=\"\\t\", header=True, index=False)\n",
        "\n",
        "        print(f\"Updated CSV file '{csv_filename}' with missing Wikipedia page details.\")\n",
        "    else:\n",
        "        print(\"No new data to update in the CSV.\")\n",
        "\n",
        "    # Remove entries from the CSV based on remove_wiki_page_list\n",
        "    if remove_wiki_page_list:\n",
        "        existing_df = existing_df[~existing_df['title'].isin(remove_wiki_page_list)]\n",
        "        existing_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "        print(f\"Removed specified entries from CSV.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d203ed00-8048-4e8b-bdfd-19736c8786ce",
      "metadata": {
        "id": "d203ed00-8048-4e8b-bdfd-19736c8786ce"
      },
      "outputs": [],
      "source": [
        "# Example usage with a list of wiki_page names to update and remove\n",
        "wiki_page_list = [\n",
        "    \"BRCA_mutation\",\n",
        "    \"BRCA1\",\n",
        "    \"BRCA2\",\n",
        "    \"Tumor_suppressor_genes\",\n",
        "    \"Adenomatous_polyposis_coli\",\n",
        "    \"ATOH1\",\n",
        "    \"BCL10\",\n",
        "    \"Cadherin-1\",\n",
        "    \"Capicua_(protein)\",\n",
        "    \"CDKN1B\",\n",
        "    \"CHEK2\",\n",
        "    \"Cyclin-dependent_kinase_inhibitor_1C\",\n",
        "    \"DHX15\",\n",
        "    \"DLD/NP1\",\n",
        "    \"Protein\",\n",
        "    \"ABL_(gene)\",\n",
        "    \"AKT1\",\n",
        "    \"Androgen_receptor\",\n",
        "    \"Ataxia_telangiectasia_and_Rad3_related\",\n",
        "    \"threonine_kinase\",\n",
        "    \"ATF1\",\n",
        "    \"BACH1\",\n",
        "    \"BARD1\",\n",
        "    \"BRCC3\",\n",
        "    \"BRE_(gene)\",\n",
        "    \"BRIP1\",\n",
        "    \"Transcription_factor_Jun\",\n",
        "    \"CHEK2\",\n",
        "    \"CLSPN\",\n",
        "    \"Cofactor_of_BRCA1\",\n",
        "    \"CREB-binding_protein\",\n",
        "    \"CSNK2B\",\n",
        "    \"CSTF2\",\n",
        "    \"Cyclin-dependent_kinase_2\",\n",
        "    \"RNA_Helicase_A\",\n",
        "    \"ELK4\",\n",
        "    \"EP300\",\n",
        "    \"Estrogen_receptor_alpha\",\n",
        "    \"FANCA\",\n",
        "    \"FANCD2\",\n",
        "    \"FHL2\",\n",
        "    \"H2AFX\",\n",
        "    \"JUNB\",\n",
        "    \"JunD\",\n",
        "    \"LMO4\",\n",
        "    \"MAP3K3\",\n",
        "    \"MED1\",\n",
        "    \"MED17\",\n",
        "    \"MED21\",\n",
        "    \"MED24\",\n",
        "    \"MRE11A\",\n",
        "    \"MSH2\",\n",
        "    \"MSH3\",\n",
        "    \"MSH6\"\n",
        "#     # Add more wiki_page names as needed\n",
        "]\n",
        "\n",
        "wiki_page_list = list(set(wiki_page_list))\n",
        "remove_wiki_page_list = []\n",
        "    # \"BRCA1\",\n",
        "    # \"BRCA2\",\n",
        "    # \"Tumor_suppressor_genes\",\n",
        "    # \"Adenomatous_polyposis_coli\",\n",
        "    # \"ATOH1\",\n",
        "    # \"BCL10\",\n",
        "    # \"Cadherin-1\",\n",
        "    # \"Capicua_(protein)\",\n",
        "    # \"CDKN1B\"]\n",
        "    # Add more page titles to remove as needed\n",
        "# ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612ee7e1-1fac-46f9-bbd5-6b88a7502514",
      "metadata": {
        "id": "612ee7e1-1fac-46f9-bbd5-6b88a7502514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f223317-ec0e-424f-a54a-cbe8ec1a2819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text from 'https://en.wikipedia.org/wiki/Cadherin-1' and saved to 'Cadherin-1.txt'.\n",
            "Downloaded 'Cadherin-1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Androgen_receptor' and saved to 'Androgen_receptor.txt'.\n",
            "Downloaded 'Androgen_receptor' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/ATOH1' and saved to 'ATOH1.txt'.\n",
            "Downloaded 'ATOH1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Cyclin-dependent_kinase_inhibitor_1C' and saved to 'Cyclin-dependent_kinase_inhibitor_1C.txt'.\n",
            "Downloaded 'Cyclin-dependent_kinase_inhibitor_1C' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/CSTF2' and saved to 'CSTF2.txt'.\n",
            "Downloaded 'CSTF2' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Protein' and saved to 'Protein.txt'.\n",
            "Downloaded 'Protein' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/EP300' and saved to 'EP300.txt'.\n",
            "Downloaded 'EP300' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/ELK4' and saved to 'ELK4.txt'.\n",
            "Downloaded 'ELK4' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BACH1' and saved to 'BACH1.txt'.\n",
            "Downloaded 'BACH1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Ataxia_telangiectasia_and_Rad3_related' and saved to 'Ataxia_telangiectasia_and_Rad3_related.txt'.\n",
            "Downloaded 'Ataxia_telangiectasia_and_Rad3_related' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Adenomatous_polyposis_coli' and saved to 'Adenomatous_polyposis_coli.txt'.\n",
            "Downloaded 'Adenomatous_polyposis_coli' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/CDKN1B' and saved to 'CDKN1B.txt'.\n",
            "Downloaded 'CDKN1B' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/AKT1' and saved to 'AKT1.txt'.\n",
            "Downloaded 'AKT1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BRCC3' and saved to 'BRCC3.txt'.\n",
            "Downloaded 'BRCC3' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/RNA_Helicase_A' and saved to 'RNA_Helicase_A.txt'.\n",
            "Downloaded 'RNA_Helicase_A' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Tumor_suppressor_genes' and saved to 'Tumor_suppressor_genes.txt'.\n",
            "Downloaded 'Tumor_suppressor_genes' and created DataFrame.\n",
            "Error extracting text: [Errno 2] No such file or directory: 'DLD/NP1.txt'\n",
            "Failed to download 'DLD/NP1'.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BRCA_mutation' and saved to 'BRCA_mutation.txt'.\n",
            "Downloaded 'BRCA_mutation' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Cofactor_of_BRCA1' and saved to 'Cofactor_of_BRCA1.txt'.\n",
            "Downloaded 'Cofactor_of_BRCA1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/CSNK2B' and saved to 'CSNK2B.txt'.\n",
            "Downloaded 'CSNK2B' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/FHL2' and saved to 'FHL2.txt'.\n",
            "Downloaded 'FHL2' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BRCA1' and saved to 'BRCA1.txt'.\n",
            "Downloaded 'BRCA1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/ATF1' and saved to 'ATF1.txt'.\n",
            "Downloaded 'ATF1' and created DataFrame.\n",
            "Error extracting text: 404 Client Error: Not Found for url: https://en.wikipedia.org/wiki/Threonine_kinase\n",
            "Failed to download 'threonine_kinase'.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/CLSPN' and saved to 'CLSPN.txt'.\n",
            "Downloaded 'CLSPN' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BRE_(gene)' and saved to 'BRE_(gene).txt'.\n",
            "Downloaded 'BRE_(gene)' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/FANCA' and saved to 'FANCA.txt'.\n",
            "Downloaded 'FANCA' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/ABL_(gene)' and saved to 'ABL_(gene).txt'.\n",
            "Downloaded 'ABL_(gene)' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/FANCD2' and saved to 'FANCD2.txt'.\n",
            "Downloaded 'FANCD2' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Cyclin-dependent_kinase_2' and saved to 'Cyclin-dependent_kinase_2.txt'.\n",
            "Downloaded 'Cyclin-dependent_kinase_2' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BCL10' and saved to 'BCL10.txt'.\n",
            "Downloaded 'BCL10' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/CREB-binding_protein' and saved to 'CREB-binding_protein.txt'.\n",
            "Downloaded 'CREB-binding_protein' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BRCA2' and saved to 'BRCA2.txt'.\n",
            "Downloaded 'BRCA2' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Capicua_(protein)' and saved to 'Capicua_(protein).txt'.\n",
            "Downloaded 'Capicua_(protein)' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/DHX15' and saved to 'DHX15.txt'.\n",
            "Downloaded 'DHX15' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/CHEK2' and saved to 'CHEK2.txt'.\n",
            "Downloaded 'CHEK2' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BRIP1' and saved to 'BRIP1.txt'.\n",
            "Downloaded 'BRIP1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/BARD1' and saved to 'BARD1.txt'.\n",
            "Downloaded 'BARD1' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Transcription_factor_Jun' and saved to 'Transcription_factor_Jun.txt'.\n",
            "Downloaded 'Transcription_factor_Jun' and created DataFrame.\n",
            "Extracted text from 'https://en.wikipedia.org/wiki/Estrogen_receptor_alpha' and saved to 'Estrogen_receptor_alpha.txt'.\n",
            "Downloaded 'Estrogen_receptor_alpha' and created DataFrame.\n",
            "Updated CSV file 'wikipedia_details.tsv' with missing Wikipedia page details.\n"
          ]
        }
      ],
      "source": [
        "# Define the source file path\n",
        "source_file = 'wikipedia_details.tsv'\n",
        "if os.path.exists(source_file):\n",
        "  os.remove(source_file)\n",
        "update_csv_with_wikipedia_details(wiki_page_list, source_file, remove_wiki_page_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "shutil.copy(source_file, destination_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0u3ESr1a5CiF",
        "outputId": "e3474fff-7766-4490-acb7-2d08cb301c8f"
      },
      "id": "0u3ESr1a5CiF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/bio-llm/wikipedia_details.tsv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EZl59KFmXiqH",
        "outputId": "18dbecc7-fd7d-4e85-ad86-d8b4177b7f40"
      },
      "id": "EZl59KFmXiqH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipedia_details.tsv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "destination_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wEC0_SumXoSQ",
        "outputId": "95b15d62-bb98-4ea5-8f00-96c1f4b413bc"
      },
      "id": "wEC0_SumXoSQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/bio-llm/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls -lt"
      ],
      "metadata": {
        "id": "rM1YYuCcXj4Y"
      },
      "id": "rM1YYuCcXj4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wHSUblTyZGs1"
      },
      "id": "wHSUblTyZGs1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}